Itâ€™s built using **pure tensor operations** to help you deeply understand how GPT models work under the hood â€” including self-attention, feedforward networks, layer norm, and more.

---

## ğŸ“ Project Structure

| File / Path           | Description |
|-----------------------|-------------|
| `gpt.py`              | ğŸ§  Main script that trains the GPT model (decoder-only) from scratch using low-level PyTorch |
| `gpt_debug.ipynb`     | ğŸ§ª Jupyter notebook for debugging and playing with tensor shapes |
| `sampling.txt`        | ğŸ“„ Example output generated from the trained neural network |
| `attention.pdf`       | ğŸ“š "Attention Is All You Need" paper (original Transformer paper) for reference |

---
- âœ… **CUDA support** â€” added using GPT ;)
