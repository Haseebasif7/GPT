It’s built using **pure tensor operations** to help you deeply understand how GPT models work under the hood — including self-attention, feedforward networks, layer norm, and more.

---

## 📁 Project Structure

| File / Path           | Description |
|-----------------------|-------------|
| `gpt.py`              | 🧠 Main script that trains the GPT model (decoder-only) from scratch using low-level PyTorch |
| `gpt_debug.ipynb`     | 🧪 Jupyter notebook for debugging and playing with tensor shapes |
| `sampling.txt`        | 📄 Example output generated from the trained neural network |
| `attention.pdf`       | 📚 "Attention Is All You Need" paper (original Transformer paper) for reference |

---
- ✅ **CUDA support** — added using GPT ;)
